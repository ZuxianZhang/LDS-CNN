import datetime
import logging
import time
from keras.layers import Reshape
from keras.utils.np_utils import to_categorical
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Input, Conv1D, GlobalMaxPooling1D, Dense
import numpy as np
import os
import argparse
from collections import Counter
from tensorflow.keras.callbacks import ModelCheckpoint
import tensorflow as tf
import json
import re
from sklearn.metrics import roc_auc_score, roc_curve, precision_recall_curve, f1_score, recall_score, auc

gpus = tf.config.list_physical_devices('GPU')
if gpus:
    try:
        for gpu in gpus:
            tf.config.experimental.set_memory_growth(gpu, True)
    except RuntimeError as e:
        print(e)

log_folder = "logs_history_CNN"
if not os.path.exists(log_folder):
    os.makedirs(log_folder)

log_filename = datetime.datetime.now().strftime("model3_final%Y%m%d.log")
logging.basicConfig(filename=os.path.join(log_folder, log_filename), level=logging.INFO)

parser = argparse.ArgumentParser(description='Train an CNN model for binary classification')
parser.add_argument('--max_len', type=int, default=1000)
parser.add_argument('--Input_shape', default=(1000, 1), help='Input layer shape')
parser.add_argument('--Conv1D_filters', type=int, default=128, help='Conv1D_1,2 layer filters')
parser.add_argument('--Conv1D_filters2', type=int, default=256, help='Conv1D_3,4 layer filters')
parser.add_argument('--epochs', type=int, default=1500, help='Number of epochs to train the model')
parser.add_argument('--batch_size', type=int, default=256, help='Batch size for training')
parser.add_argument('--loss', type=str, default='binary_crossentropy', help='loss ')
parser.add_argument('--optimizer', type=str, default='adam', help='Optimizer to use')

args = parser.parse_args()

model_name = 'CNN'
max_len = args.max_len
Input_shape = args.Input_shape
Conv1D_filters = args.Conv1D_filters
Conv1D_filters2 = args.Conv1D_filters2
epochs = args.epochs
batch_size = args.batch_size
loss = args.loss
optimizer = args.optimizer

logging.info("Start time:{}".format(time.time()))
logging.info("Model hyperparameters:")
logging.info('Model_name: {}'.format(model_name))
logging.info('max_len: {}'.format(max_len))
logging.info('Input_shape: {}'.format(Input_shape))
logging.info('Conv1D_filters: {}'.format(Conv1D_filters))
logging.info('Conv1D_filters2: {}'.format(Conv1D_filters2))
logging.info('Epochs: {}'.format(epochs))
logging.info('Batch_size: {}'.format(batch_size))
logging.info('loss: {}'.format(loss))
logging.info('Optimizer: {}'.format(optimizer))

protein_dict = {}
with open('', 'r') as b_file:
    for line in b_file:
        parts = line.rstrip().split(',')
        protein_dict[parts[0]] = parts[1]

chemical_dict = {}
with open('', 'r') as b_file:
    for line in b_file:
        key = line.rstrip().split(',')[0]
        value = line.rstrip().split(',')[1]
        chemical_dict[key] = value

pos_data = []
with open('', 'r') as c_file:
    for line in c_file:
        parts = line.strip().split(',')
        if parts[3] == '1':
            if parts[0] in chemical_dict:
                pos_data.append(chemical_dict[parts[0]] + ',' + protein_dict[parts[1]])
        elif parts[3] == '2':
            if parts[0] in protein_dict:
                pos_data.append(protein_dict[parts[0]] + ',' + chemical_dict[parts[1]])
        elif parts[3] == '3':
            if parts[0] in chemical_dict and parts[1] in chemical_dict:
                pos_data.append(chemical_dict[parts[0]] + ',' + chemical_dict[parts[1]])

neg_data = []
with open('', 'r') as c_file:
    for line in c_file:
        parts = line.strip().split(',')
        if parts[2] == '1':
            if parts[0] in chemical_dict:
                neg_data.append(chemical_dict[parts[0]] + ',' + protein_dict[parts[1]])
        elif parts[2] == '2':
            if parts[0] in protein_dict:
                neg_data.append(protein_dict[parts[0]] + ',' + chemical_dict[parts[1]])
        elif parts[2] == '3':
            if parts[0] in chemical_dict and parts[1] in chemical_dict:
                neg_data.append(chemical_dict[parts[0]] + ',' + chemical_dict[parts[1]])

MAX_AA_LEN = 600
MAX_SMILES_LEN = 400

AA_REGEX = re.compile(r'^[A-Z]+$')
SMILES_REGEX = re.compile(r'^[A-Za-z0-9@+\\\/\-\[\]\(\)\.#=]+$', re.IGNORECASE)

new_pos_data = []
new_neg_data = []

for data in pos_data:
    seq_a, seq_b = data.strip().split(',')
    if AA_REGEX.match(seq_a):
        if SMILES_REGEX.match(seq_b) and len(seq_a) <= MAX_AA_LEN and len(seq_b) <= MAX_SMILES_LEN:
            new_pos_data.append(seq_a + seq_b)
    elif SMILES_REGEX.match(seq_a):
        if AA_REGEX.match(seq_b) and len(seq_a) <= MAX_SMILES_LEN and len(seq_b) <= MAX_AA_LEN:
            new_pos_data.append(seq_a + seq_b)

for data in neg_data:
    seq_a, seq_b = data.strip().split(',')
    if AA_REGEX.match(seq_a):
        if SMILES_REGEX.match(seq_b) and len(seq_a) <= MAX_AA_LEN and len(seq_b) <= MAX_SMILES_LEN:
            new_neg_data.append(seq_a + seq_b)
    elif SMILES_REGEX.match(seq_a):
        if AA_REGEX.match(seq_b) and len(seq_a) <= MAX_SMILES_LEN and len(seq_b) <= MAX_AA_LEN:
            new_neg_data.append(seq_a + seq_b)


print('new_pos_data num:{}'.format(len(new_pos_data)))  
print('new_neg_data num:{}'.format(len(new_neg_data)))   


max_len = 1000
padded_pos_data = [] 
for s in new_pos_data:
    padded_s = s.ljust(max_len, " ") 
    padded_pos_data.append(padded_s)

padded_neg_data = []  
for s in new_neg_data:
    padded_s = s.ljust(max_len, " ")  
    padded_neg_data.append(padded_s)

pos_text = padded_pos_data 
neg_text = padded_neg_data
n = len(set(''.join(neg_text)))  
counts = Counter(''.join(neg_text))  
code_dict = {char: i/n for i, char in enumerate(sorted(counts.keys()))}  

with open('','w') as f:
     json.dump(code_dict, f)

with open('', 'r') as f:
    code_dict = json.load(f)

encoded_pos_text = []
encoded_neg_text = []
for s in pos_text:
    encoded_s = [code_dict[char] for char in s]
    encoded_pos_text.append(encoded_s)
for s in neg_text:
    encoded_s = [code_dict[char] for char in s]
    encoded_neg_text.append(encoded_s)

from sklearn.model_selection import train_test_split

X = encoded_pos_text + encoded_neg_text
y = [1] * len(encoded_pos_text) + [0] * len(encoded_neg_text)

X_train_val, X_test, y_train_val, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
X_train, X_val, y_train, y_val = train_test_split(X_train_val, y_train_val, test_size=0.25, random_state=42)

train_set = (X_train, y_train)
val_set = (X_val, y_val)
test_set = (X_test, y_test)

x_train = np.array(X_train)
x_train = np.expand_dims(x_train, axis=-1)  
x_val = np.array(X_val)
x_val = np.expand_dims(x_val, axis=-1)  

labels = np.array(y_train)
labels = to_categorical(labels)

np.random.seed(28)
indices = np.arange(x_train.shape[0])
np.random.shuffle(indices)
x_train = x_train[indices]
labels = labels[indices]

labels1 = np.array(y_val)
labels1 = to_categorical(labels1)

np.random.seed(28)
indices = np.arange(x_val.shape[0])
np.random.shuffle(indices)
x_val = x_val[indices]
labels1 = labels1[indices]

y_train = labels
y_val = labels1

print('Train data = {} '.format(len(x_train)))
print('Val data = {}'.format(len(x_val)))
